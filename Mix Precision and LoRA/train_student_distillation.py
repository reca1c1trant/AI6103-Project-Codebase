"""
Step 2: Knowledge Distillation - Train Student BERT-12 with Teacher BERT-12

ç”¨æ³•:
torchrun --nproc_per_node=8 train_student_distillation.py     --teacher_path google-bert/bert-large-uncased-whole-word-masking-finetuned-squad     --student_model bert-base-uncased     --train_path SQuAD-v1.1.csv     --num_epochs 6     --batch_size 16     --learning_rate 3e-5     --alpha 0.5     --precision bf16     --output_dir ./outputs_student_distilled_bf16

"""

import os
import time
import torch
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader, DistributedSampler, Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForQuestionAnswering,
    get_cosine_schedule_with_warmup,
    set_seed
)
from torch.amp import autocast, GradScaler
from tqdm import tqdm
import argparse
import pandas as pd
import json
import glob
import shutil

class SQuADQADataset(Dataset):
    """SQuAD QAæ•°æ®é›†"""
    def __init__(self, encodings):
        self.encodings = encodings
    
    def __len__(self):
        return len(self.encodings['input_ids'])
    
    def __getitem__(self, idx):
        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}

class DistillationTrainer:
    def __init__(self, args):
        self.args = args
        self.setup_distributed()
        self.setup_model_and_data()
        self.setup_training()
        
    def setup_distributed(self):
        """è®¾ç½®åˆ†å¸ƒå¼è®­ç»ƒ"""
        if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:
            self.rank = int(os.environ["RANK"])
            self.world_size = int(os.environ['WORLD_SIZE'])
            self.local_rank = int(os.environ['LOCAL_RANK'])
        else:
            self.rank = 0
            self.world_size = 1
            self.local_rank = 0
            
        torch.cuda.set_device(self.local_rank)
        
        if self.world_size > 1:
            dist.init_process_group(backend='nccl')
        
        self.is_main = self.rank == 0
        
        if self.is_main:
            print(f"\n{'='*70}")
            print(f" STEP 2: Knowledge Distillation (12-layer â†’ 12-layer)")
            print(f"{'='*70}")
            print(f" åˆ†å¸ƒå¼è®­ç»ƒè®¾ç½®:")
            print(f"   - World Size: {self.world_size}")
            print(f"   - Rank: {self.rank}")
            print(f"   - Local Rank: {self.local_rank}")
    
    def load_squad_qa_data(self, csv_path):
        """ä»CSVåŠ è½½SQuAD QAæ•°æ®"""
        if self.is_main:
            print(f"\n åŠ è½½æ•°æ®: {csv_path}")
        
        df = pd.read_csv(csv_path)
        df = df.dropna(subset=['context', 'question', 'answer'])
        
        if self.is_main:
            print(f"   - æœ‰æ•ˆæ ·æœ¬: {len(df):,}")
        
        contexts = df['context'].tolist()
        questions = df['question'].tolist()
        answers = df['answer'].tolist()
        
        return contexts, questions, answers
    
    def prepare_qa_features(self, contexts, questions, answers, tokenizer):
        """å‡†å¤‡QAç‰¹å¾"""
        encodings = {
            'input_ids': [],
            'attention_mask': [],
            'start_positions': [],
            'end_positions': []
        }
        
        if self.is_main:
            print(f"\n Tokenization...")
        
        for context, question, answer in tqdm(
            zip(contexts, questions, answers),
            total=len(contexts),
            disable=not self.is_main,
            desc="Encoding"
        ):
            encoding = tokenizer(
                question,
                context,
                max_length=self.args.max_length,
                truncation=True,
                padding='max_length',
                return_offsets_mapping=True,
                return_tensors='pt'
            )
            
            answer_start = context.find(answer)
            if answer_start == -1:
                continue
            
            answer_end = answer_start + len(answer)
            offset_mapping = encoding['offset_mapping'][0].tolist()
            
            start_position = 0
            end_position = 0
            
            for idx, (start, end) in enumerate(offset_mapping):
                if start <= answer_start < end:
                    start_position = idx
                if start < answer_end <= end:
                    end_position = idx
                    break
            
            encodings['input_ids'].append(encoding['input_ids'][0].tolist())
            encodings['attention_mask'].append(encoding['attention_mask'][0].tolist())
            encodings['start_positions'].append(start_position)
            encodings['end_positions'].append(end_position)
        
        if self.is_main:
            print(f"    ç¼–ç å®Œæˆ: {len(encodings['input_ids']):,} æ ·æœ¬")
        
        return encodings
    
    def setup_model_and_data(self):
        """åŠ è½½Teacherå’ŒStudentæ¨¡å‹"""
        # åŠ è½½tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(self.args.teacher_path)
        
        # åŠ è½½è®­ç»ƒæ•°æ®
        train_contexts, train_questions, train_answers = self.load_squad_qa_data(
            self.args.train_path
        )
        
        train_encodings = self.prepare_qa_features(
            train_contexts, train_questions, train_answers, self.tokenizer
        )
        self.train_dataset = SQuADQADataset(train_encodings)
        
        # éªŒè¯é›†
        if self.args.eval_path:
            eval_contexts, eval_questions, eval_answers = self.load_squad_qa_data(
                self.args.eval_path
            )
            eval_encodings = self.prepare_qa_features(
                eval_contexts, eval_questions, eval_answers, self.tokenizer
            )
            self.eval_dataset = SQuADQADataset(eval_encodings)
        else:
            train_size = int(len(train_encodings['input_ids']) * 0.9)
            train_enc = {k: v[:train_size] for k, v in train_encodings.items()}
            eval_enc = {k: v[train_size:] for k, v in train_encodings.items()}
            
            self.train_dataset = SQuADQADataset(train_enc)
            self.eval_dataset = SQuADQADataset(eval_enc)
            
            if self.is_main:
                print(f"   - è®­ç»ƒé›†: {len(self.train_dataset):,}")
                print(f"   - éªŒè¯é›†: {len(self.eval_dataset):,}")
        
        # åŠ è½½Teacheræ¨¡å‹ï¼ˆå†»ç»“ï¼‰
        if self.is_main:
            print(f"\n åŠ è½½Teacheræ¨¡å‹: {self.args.teacher_path}")
        
        self.teacher = AutoModelForQuestionAnswering.from_pretrained(
            self.args.teacher_path
        ).to(self.local_rank)
        self.teacher.eval()  # è®¾ä¸ºevalæ¨¡å¼
        
        # å†»ç»“Teacherå‚æ•°
        for param in self.teacher.parameters():
            param.requires_grad = False
        
        if self.is_main:
            print(f"   - æ¨¡å‹ç±»å‹: BERT-base (Teacher)")
            print(f"   - å±‚æ•°: {self.teacher.config.num_hidden_layers}")
            print(f"   - çŠ¶æ€: å†»ç»“ (ç”¨äºç”Ÿæˆsoft labels)")
        
        # åŠ è½½Studentæ¨¡å‹ï¼ˆè®­ç»ƒï¼‰
        if self.is_main:
            print(f"\n åŠ è½½Studentæ¨¡å‹: {self.args.student_model}")
        
        self.student = AutoModelForQuestionAnswering.from_pretrained(
            self.args.student_model
        ).to(self.local_rank)
        
        if self.is_main:
            print(f"   - æ¨¡å‹ç±»å‹: BERT-base (Student)")
            print(f"   - å±‚æ•°: {self.student.config.num_hidden_layers}")
            print(f"   - çŠ¶æ€: å¯è®­ç»ƒ")
        
        # DDPåŒ…è£…ï¼ˆåªåŒ…è£…Studentï¼‰
        if self.world_size > 1:
            self.student = DDP(
                self.student,
                device_ids=[self.local_rank],
                output_device=self.local_rank,
                find_unused_parameters=False
            )
        
        if self.is_main:
            student_params = sum(p.numel() for p in self.student.parameters())
            trainable_params = sum(p.numel() for p in self.student.parameters() if p.requires_grad)
            print(f"   - Studentå‚æ•°é‡: {student_params:,}")
            print(f"   - å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    
    def setup_training(self):
        """è®¾ç½®è®­ç»ƒç»„ä»¶"""
        # DataLoader
        if self.world_size > 1:
            train_sampler = DistributedSampler(
                self.train_dataset,
                num_replicas=self.world_size,
                rank=self.rank,
                shuffle=True,
                seed=42
            )
            eval_sampler = DistributedSampler(
                self.eval_dataset,
                num_replicas=self.world_size,
                rank=self.rank,
                shuffle=False
            )
        else:
            train_sampler = None
            eval_sampler = None
        
        self.train_loader = DataLoader(
            self.train_dataset,
            batch_size=self.args.batch_size,
            sampler=train_sampler,
            shuffle=(train_sampler is None),
            num_workers=4,
            pin_memory=True
        )
        
        self.eval_loader = DataLoader(
            self.eval_dataset,
            batch_size=self.args.batch_size * 2,
            sampler=eval_sampler,
            num_workers=4,
            pin_memory=True
        )
        
        # ä¼˜åŒ–å™¨ï¼ˆåªä¼˜åŒ–Studentï¼‰
        no_decay = ['bias', 'LayerNorm.weight']
        optimizer_grouped_parameters = [
            {
                'params': [p for n, p in self.student.named_parameters() 
                          if not any(nd in n for nd in no_decay)],
                'weight_decay': self.args.weight_decay,
            },
            {
                'params': [p for n, p in self.student.named_parameters() 
                          if any(nd in n for nd in no_decay)],
                'weight_decay': 0.0,
            }
        ]
        
        self.optimizer = torch.optim.AdamW(
            optimizer_grouped_parameters,
            lr=self.args.learning_rate,
            betas=(0.9, 0.999),
            eps=1e-8
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        num_training_steps = len(self.train_loader) * self.args.num_epochs
        num_warmup_steps = int(num_training_steps * self.args.warmup_ratio)
        
        self.scheduler = get_cosine_schedule_with_warmup(
            self.optimizer,
            num_warmup_steps=num_warmup_steps,
            num_training_steps=num_training_steps
        )
        
        # Mixed Precision
        self.setup_mixed_precision()
        
        if self.is_main:
            print(f"\n  çŸ¥è¯†è’¸é¦é…ç½®:")
            print(f"   - Temperature: {self.args.temperature}")
            print(f"   - Alpha (hard loss weight): {self.args.alpha}")
            print(f"   - 1-Alpha (distillation loss weight): {1-self.args.alpha}")
            print(f"   - Precision: {self.args.precision}")
            print(f"   - Batch Size (per device): {self.args.batch_size}")
            print(f"   - Global Batch Size: {self.args.batch_size * self.world_size}")
            print(f"   - å­¦ä¹ ç‡: {self.args.learning_rate}")
            print(f"   - å­¦ä¹ ç‡è°ƒåº¦: Cosine Annealing with warmup")
            print(f"   - Warmupæ­¥æ•°: {num_warmup_steps:,} ({self.args.warmup_ratio*100}%)")
            print(f"   - æ€»è®­ç»ƒæ­¥æ•°: {num_training_steps:,}")
    
    def setup_mixed_precision(self):
        """è®¾ç½®Mixed Precision"""
        self.use_amp = False
        self.scaler = None
        self.amp_dtype = torch.float32
        
        if self.args.precision == "fp16":
            self.use_amp = True
            self.amp_dtype = torch.float16
            self.scaler = GradScaler()
            if self.is_main:
                print(f"    å¯ç”¨ FP16 è®­ç»ƒ")
        elif self.args.precision == "bf16":
            if torch.cuda.is_bf16_supported():
                self.use_amp = True
                self.amp_dtype = torch.bfloat16
                if self.is_main:
                    print(f"    å¯ç”¨ BF16 è®­ç»ƒ")
            else:
                if self.is_main:
                    print(f"     GPUä¸æ”¯æŒBF16ï¼Œå›é€€åˆ°FP32")
        elif self.args.precision == "fp32":
            if self.is_main:
                print(f"    ä½¿ç”¨ FP32 è®­ç»ƒ")
    
    def distillation_loss(self, student_logits, teacher_logits, temperature):
        """
        è®¡ç®—è’¸é¦æŸå¤± (KLæ•£åº¦)
        
        Args:
            student_logits: Studentæ¨¡å‹çš„logits (start_logits, end_logits)
            teacher_logits: Teacheræ¨¡å‹çš„logits (start_logits, end_logits)
            temperature: æ¸©åº¦å‚æ•°
        """
        student_start, student_end = student_logits
        teacher_start, teacher_end = teacher_logits
        
        # å¯¹start positionçš„KLæ•£åº¦
        start_loss = F.kl_div(
            F.log_softmax(student_start / temperature, dim=-1),
            F.softmax(teacher_start / temperature, dim=-1),
            reduction='batchmean'
        ) * (temperature ** 2)
        
        # å¯¹end positionçš„KLæ•£åº¦
        end_loss = F.kl_div(
            F.log_softmax(student_end / temperature, dim=-1),
            F.softmax(teacher_end / temperature, dim=-1),
            reduction='batchmean'
        ) * (temperature ** 2)
        
        return (start_loss + end_loss) / 2
    
    def train_epoch(self, epoch):
        """è®­ç»ƒä¸€ä¸ªepochï¼ˆå¸¦çŸ¥è¯†è’¸é¦ï¼‰"""
        self.student.train()
        self.teacher.eval()  # Teacherå§‹ç»ˆä¿æŒeval
        
        total_loss = 0
        total_hard_loss = 0
        total_distill_loss = 0
        
        if self.world_size > 1:
            self.train_loader.sampler.set_epoch(epoch)
        
        pbar = tqdm(self.train_loader, disable=not self.is_main, 
                   desc=f"Epoch {epoch+1}/{self.args.num_epochs}")
        
        for batch in pbar:
            batch = {k: v.to(self.local_rank) for k, v in batch.items()}
            
            # Forward pass
            if self.use_amp:
                with autocast(device_type='cuda', dtype=self.amp_dtype, enabled=True):
                    # Student forward
                    student_outputs = self.student(**batch)
                    hard_loss = student_outputs.loss
                    
                    # Teacher forwardï¼ˆæ— æ¢¯åº¦ï¼‰
                    with torch.no_grad():
                        teacher_outputs = self.teacher(
                            input_ids=batch['input_ids'],
                            attention_mask=batch['attention_mask']
                        )
                    
                    # è®¡ç®—è’¸é¦æŸå¤±
                    distill_loss = self.distillation_loss(
                        (student_outputs.start_logits, student_outputs.end_logits),
                        (teacher_outputs.start_logits, teacher_outputs.end_logits),
                        self.args.temperature
                    )
                    
                    # ç»„åˆæŸå¤±
                    loss = self.args.alpha * hard_loss + (1 - self.args.alpha) * distill_loss
            else:
                # Student forward
                student_outputs = self.student(**batch)
                hard_loss = student_outputs.loss
                
                # Teacher forwardï¼ˆæ— æ¢¯åº¦ï¼‰
                with torch.no_grad():
                    teacher_outputs = self.teacher(
                        input_ids=batch['input_ids'],
                        attention_mask=batch['attention_mask']
                    )
                
                # è®¡ç®—è’¸é¦æŸå¤±
                distill_loss = self.distillation_loss(
                    (student_outputs.start_logits, student_outputs.end_logits),
                    (teacher_outputs.start_logits, teacher_outputs.end_logits),
                    self.args.temperature
                )
                
                # ç»„åˆæŸå¤±
                loss = self.args.alpha * hard_loss + (1 - self.args.alpha) * distill_loss
            
            # Backward pass
            if self.scaler is not None:
                self.scaler.scale(loss).backward()
                self.scaler.unscale_(self.optimizer)
                torch.nn.utils.clip_grad_norm_(self.student.parameters(), 
                                              self.args.max_grad_norm)
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.student.parameters(), 
                                              self.args.max_grad_norm)
                self.optimizer.step()
            
            self.scheduler.step()
            self.optimizer.zero_grad()
            
            total_loss += loss.item()
            total_hard_loss += hard_loss.item()
            total_distill_loss += distill_loss.item()
            
            if self.is_main:
                pbar.set_postfix({
                    'loss': f"{loss.item():.4f}",
                    'hard': f"{hard_loss.item():.4f}",
                    'distill': f"{distill_loss.item():.4f}",
                    'lr': f"{self.scheduler.get_last_lr()[0]:.2e}"
                })
        
        avg_loss = total_loss / len(self.train_loader)
        avg_hard_loss = total_hard_loss / len(self.train_loader)
        avg_distill_loss = total_distill_loss / len(self.train_loader)
        
        return avg_loss, avg_hard_loss, avg_distill_loss
    
    def evaluate(self):
        """è¯„ä¼°Studentæ¨¡å‹"""
        self.student.eval()
        total_loss = 0
        
        with torch.no_grad():
            for batch in tqdm(self.eval_loader, disable=not self.is_main, desc="Eval"):
                batch = {k: v.to(self.local_rank) for k, v in batch.items()}
                
                if self.use_amp:
                    with autocast(device_type='cuda', dtype=self.amp_dtype, enabled=True):
                        outputs = self.student(**batch)
                        loss = outputs.loss
                else:
                    outputs = self.student(**batch)
                    loss = outputs.loss
                
                total_loss += loss.item()
        
        avg_loss = total_loss / len(self.eval_loader)
        
        # å¤šå¡åŒæ­¥
        if self.world_size > 1:
            avg_loss_tensor = torch.tensor(avg_loss).to(self.local_rank)
            dist.all_reduce(avg_loss_tensor, op=dist.ReduceOp.SUM)
            avg_loss = avg_loss_tensor.item() / self.world_size
        
        return avg_loss
    
    def delete_old_checkpoints(self):
        """åˆ é™¤æ—§checkpoint"""
        if not self.is_main:
            return
        
        checkpoint_pattern = os.path.join(self.args.output_dir, 'checkpoint-epoch-*')
        checkpoints = sorted(glob.glob(checkpoint_pattern))
        
        if len(checkpoints) > 1:
            for old_checkpoint in checkpoints[:-1]:
                if self.is_main:
                    print(f"   ğŸ—‘ï¸  åˆ é™¤æ—§checkpoint: {old_checkpoint}")
                shutil.rmtree(old_checkpoint)
    
    def save_checkpoint(self, epoch, eval_loss, is_best=False):
        """ä¿å­˜checkpoint"""
        if not self.is_main:
            return
        
        self.delete_old_checkpoints()
        
        # è·å–çœŸå®çš„studentæ¨¡å‹ï¼ˆå»é™¤DDP wrapperï¼‰
        student_model = self.student.module if self.world_size > 1 else self.student
        
        # ä¿å­˜æœ€æ–°checkpoint
        checkpoint_dir = os.path.join(self.args.output_dir, 
                                     f"checkpoint-epoch-{epoch+1}")
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        student_model.save_pretrained(checkpoint_dir)
        self.tokenizer.save_pretrained(checkpoint_dir)
        
        # ä¿å­˜è®­ç»ƒçŠ¶æ€
        checkpoint_state = {
            'epoch': epoch,
            'eval_loss': eval_loss,
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
        }
        
        if self.scaler is not None:
            checkpoint_state['scaler_state_dict'] = self.scaler.state_dict()
        
        torch.save(checkpoint_state, 
                  os.path.join(checkpoint_dir, 'training_state.pt'))
        
        print(f"    ä¿å­˜checkpoint: {checkpoint_dir}")
        
        # ä¿å­˜best student model
        if is_best:
            best_model_dir = os.path.join(self.args.output_dir, "best_student_model")
            os.makedirs(best_model_dir, exist_ok=True)
            
            student_model.save_pretrained(best_model_dir)
            self.tokenizer.save_pretrained(best_model_dir)
            
            best_info = {
                'epoch': epoch + 1,
                'eval_loss': eval_loss,
                'precision': self.args.precision,
                'model_type': 'student',
                'num_layers': 12,
                'teacher_path': self.args.teacher_path,
                'temperature': self.args.temperature,
                'alpha': self.args.alpha,
            }
            
            with open(os.path.join(best_model_dir, 'student_info.json'), 'w') as f:
                json.dump(best_info, f, indent=2)
            
            print(f"    ä¿å­˜æœ€ä½³Studentæ¨¡å‹: {best_model_dir}")
    
    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        start_time = time.time()
        best_eval_loss = float('inf')
        
        training_stats = {
            'model_type': 'student_distilled',
            'teacher_path': self.args.teacher_path,
            'student_model': self.args.student_model,
            'temperature': self.args.temperature,
            'alpha': self.args.alpha,
            'precision': self.args.precision,
            'batch_size': self.args.batch_size,
            'world_size': self.world_size,
            'learning_rate': self.args.learning_rate,
            'epochs': [],
        }
        
        for epoch in range(self.args.num_epochs):
            if self.is_main:
                print(f"\n{'='*70}")
                print(f" Epoch {epoch+1}/{self.args.num_epochs}")
                print(f"{'='*70}")
            
            epoch_start = time.time()
            
            # è®­ç»ƒï¼ˆå¸¦è’¸é¦ï¼‰
            train_loss, hard_loss, distill_loss = self.train_epoch(epoch)
            train_time = time.time() - epoch_start
            
            # è¯„ä¼°
            eval_start = time.time()
            eval_loss = self.evaluate()
            eval_time = time.time() - eval_start
            
            # åˆ¤æ–­æœ€ä½³
            is_best = eval_loss < best_eval_loss
            if is_best:
                best_eval_loss = eval_loss
            
            # è®°å½•ç»Ÿè®¡
            epoch_stats = {
                'epoch': epoch + 1,
                'train_loss': train_loss,
                'hard_loss': hard_loss,
                'distill_loss': distill_loss,
                'eval_loss': eval_loss,
                'train_time_minutes': train_time / 60,
                'eval_time_minutes': eval_time / 60,
                'is_best': is_best,
                'learning_rate': self.scheduler.get_last_lr()[0],
            }
            training_stats['epochs'].append(epoch_stats)
            
            if self.is_main:
                print(f"\n Epoch {epoch+1} ç»“æœ:")
                print(f"   - Total Loss:  {train_loss:.4f}")
                print(f"   - Hard Loss:   {hard_loss:.4f} (Î±={self.args.alpha})")
                print(f"   - Distill Loss: {distill_loss:.4f} (1-Î±={1-self.args.alpha})")
                print(f"   - Eval Loss:   {eval_loss:.4f} {'ğŸŒŸ (BEST)' if is_best else ''}")
                print(f"   - Train Time:  {train_time/60:.2f} min")
                print(f"   - Eval Time:   {eval_time/60:.2f} min")
                print(f"   - Learning Rate: {self.scheduler.get_last_lr()[0]:.2e}")
            
            # ä¿å­˜checkpoint
            self.save_checkpoint(epoch, eval_loss, is_best=is_best)
        
        total_time = time.time() - start_time
        training_stats['total_time_hours'] = total_time / 3600
        training_stats['best_eval_loss'] = best_eval_loss
        
        if self.is_main:
            print(f"\n{'='*70}")
            print(f" çŸ¥è¯†è’¸é¦è®­ç»ƒå®Œæˆ!")
            print(f"{'='*70}")
            print(f"   - æ€»è€—æ—¶: {total_time/3600:.2f} å°æ—¶")
            print(f"   - æœ€ä½³Eval Loss: {best_eval_loss:.4f}")
            print(f"   - Best Studentæ¨¡å‹è·¯å¾„: {os.path.join(self.args.output_dir, 'best_student_model')}")
            print(f"{'='*70}\n")
            
            stats_path = os.path.join(self.args.output_dir, 'distillation_stats.json')
            with open(stats_path, 'w') as f:
                json.dump(training_stats, f, indent=2)
            print(f" è’¸é¦è®­ç»ƒç»Ÿè®¡: {stats_path}\n")
        
        if self.world_size > 1:
            dist.destroy_process_group()


def main():
    parser = argparse.ArgumentParser(description='Step 2: Knowledge Distillation from Teacher to Student')
    
    # æ•°æ®å‚æ•°
    parser.add_argument('--train_path', type=str, required=True,
                       help='è®­ç»ƒæ•°æ®CSVè·¯å¾„')
    parser.add_argument('--eval_path', type=str, default=None,
                       help='éªŒè¯æ•°æ®CSVè·¯å¾„ï¼ˆå¯é€‰ï¼‰')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--teacher_path', type=str, required=True,
                       help='Teacheræ¨¡å‹è·¯å¾„ï¼ˆStep 1è®­ç»ƒå¥½çš„best_teacher_modelï¼‰')
    parser.add_argument('--student_model', type=str, default='bert-base-uncased',
                       help='Studentåˆå§‹æ¨¡å‹ï¼ˆ12å±‚ï¼‰')
    parser.add_argument('--max_length', type=int, default=512,
                       help='æœ€å¤§åºåˆ—é•¿åº¦')
    
    # è’¸é¦å‚æ•°
    parser.add_argument('--temperature', type=float, default=6.0,
                       help='è’¸é¦æ¸©åº¦ï¼ˆè¶Šå¤§è¶Šsoftï¼‰')
    parser.add_argument('--alpha', type=float, default=0.2,
                       help='Hard lossæƒé‡ï¼ˆ1-alphaä¸ºdistillation lossæƒé‡ï¼‰')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--num_epochs', type=int, default=10,
                       help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=16,
                       help='æ¯å¼ å¡çš„batch size')
    parser.add_argument('--learning_rate', type=float, default=3e-5,
                       help='å­¦ä¹ ç‡ï¼ˆé€šå¸¸æ¯”teacherè®­ç»ƒæ—¶ç¨å¤§ï¼‰')
    parser.add_argument('--weight_decay', type=float, default=0.01,
                       help='æƒé‡è¡°å‡')
    parser.add_argument('--max_grad_norm', type=float, default=1.0,
                       help='æ¢¯åº¦è£å‰ª')
    parser.add_argument('--warmup_ratio', type=float, default=0.1,
                       help='Warmupæ¯”ä¾‹')
    
    # Mixed Precision
    parser.add_argument('--precision', type=str, default='bf16',
                       choices=['fp32', 'fp16', 'bf16'],
                       help='è®­ç»ƒç²¾åº¦')
    
    # è¾“å‡º
    parser.add_argument('--output_dir', type=str, default='./outputs_student_distilled',
                       help='è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    
    set_seed(42)
    os.makedirs(args.output_dir, exist_ok=True)
    
    trainer = DistillationTrainer(args)
    trainer.train()


if __name__ == '__main__':
    main()


"""
torchrun --nproc_per_node=8 train_student_distillation.py \
    --teacher_path google-bert/bert-large-uncased-whole-word-masking-finetuned-squad \
    --student_model bert-base-uncased \
    --train_path SQuAD-v1.1.csv \
    --num_epochs 10 \
    --batch_size 16 \
    --learning_rate 3e-5 \
    --alpha 0.5 \
    --precision fp16 \
    --weight_decay 0.01 \
    --output_dir ./outputs_fp16

"""    